{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "name": "01 - Sentiment Analysis - Basic DNN Models.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a6d42bc8-b0d5-473d-fbda-ccc185ccec1e",
        "id": "Zu2BQBRN5jwQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "source": [
        "!pip install contractions\n",
        "!pip install textsearch\n",
        "!pip install tqdm\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/2a/ba0a3812e2a1de2cc4ee0ded0bdb750a7cef1631c13c78a4fc4ab042adec/contractions-0.0.21-py2.py3-none-any.whl\n",
            "Installing collected packages: contractions\n",
            "Successfully installed contractions-0.0.21\n",
            "Collecting textsearch\n",
            "  Downloading https://files.pythonhosted.org/packages/42/a8/03407021f9555043de5492a2bd7a35c56cc03c2510092b5ec018cae1bbf1/textsearch-0.0.17-py2.py3-none-any.whl\n",
            "Collecting Unidecode (from textsearch)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 9.4MB/s \n",
            "\u001b[?25hCollecting pyahocorasick (from textsearch)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/9f/f0d8e8850e12829eea2e778f1c90e3c53a9a799b7f412082a5d21cd19ae1/pyahocorasick-1.4.0.tar.gz (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 45.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.0-cp36-cp36m-linux_x86_64.whl size=81708 sha256=a82a241a4c14773d1e65d5f674c281ea1561877311e3f8c400acb2609709a52d\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/90/61/87a55f5b459792fbb2b7ba6b31721b06ff5cf6bde541b40994\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: Unidecode, pyahocorasick, textsearch\n",
            "Successfully installed Unidecode-1.1.1 pyahocorasick-1.4.0 textsearch-0.0.17\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UMFvLtEHRaFM"
      },
      "source": [
        "# Load and View Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qgphrYufRaFR",
        "outputId": "342b6b9a-5709-4a5a-fa67-ad69eff8c167",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataset = pd.read_csv(r'https://github.com/dipanjanS/nlp_workshop_dhs18/raw/master/Unit%2011%20-%20Sentiment%20Analysis%20-%20Unsupervised%20Learning/movie_reviews.csv.bz2')\n",
        "dataset.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 50000 entries, 0 to 49999\n",
            "Data columns (total 2 columns):\n",
            "review       50000 non-null object\n",
            "sentiment    50000 non-null object\n",
            "dtypes: object(2)\n",
            "memory usage: 781.3+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5nkEEGExRaFc",
        "outputId": "6cbdddca-80a5-49fc-e4a4-d0abb8b4c66a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-V1NWGhcRaFi"
      },
      "source": [
        "# Build Train and Test Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7JP10IEYRaFj",
        "colab": {}
      },
      "source": [
        "# build train and test datasets\n",
        "reviews = dataset['review'].values\n",
        "sentiments = dataset['sentiment'].values\n",
        "\n",
        "train_reviews = reviews[:35000]\n",
        "train_sentiments = sentiments[:35000]\n",
        "\n",
        "test_reviews = reviews[35000:]\n",
        "test_sentiments = sentiments[35000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i45AFxXNRaFn"
      },
      "source": [
        "# Text Wrangling & Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oHZ0lEGNRaFo",
        "colab": {}
      },
      "source": [
        "import contractions\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "import re\n",
        "import tqdm\n",
        "import unicodedata\n",
        "\n",
        "\n",
        "def strip_html_tags(text):\n",
        "  soup = BeautifulSoup(text, \"html.parser\")\n",
        "  [s.extract() for s in soup(['iframe', 'script'])]\n",
        "  stripped_text = soup.get_text()\n",
        "  stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
        "  return stripped_text\n",
        "\n",
        "def remove_accented_chars(text):\n",
        "  text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "  return text\n",
        "\n",
        "def pre_process_corpus(docs):\n",
        "  norm_docs = []\n",
        "  for doc in tqdm.tqdm(docs):\n",
        "    doc = strip_html_tags(doc)\n",
        "    doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n",
        "    doc = doc.lower()\n",
        "    doc = remove_accented_chars(doc)\n",
        "    doc = contractions.fix(doc)\n",
        "    # lower case and remove special characters\\whitespaces\n",
        "    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A)\n",
        "    doc = re.sub(' +', ' ', doc)\n",
        "    doc = doc.strip()  \n",
        "    norm_docs.append(doc)\n",
        "  \n",
        "  return norm_docs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CO3ESug2RaFr",
        "outputId": "b1745561-54ee-45d9-92be-e23f302cb500",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "norm_train_reviews = pre_process_corpus(train_reviews)\n",
        "norm_test_reviews = pre_process_corpus(test_reviews)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 35000/35000 [00:18<00:00, 1851.97it/s]\n",
            "100%|██████████| 15000/15000 [00:08<00:00, 1851.75it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 26.8 s, sys: 164 ms, total: 26.9 s\n",
            "Wall time: 27 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ucDv8n50RaFu"
      },
      "source": [
        "# Traditional Supervised Machine Learning Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AOZ7Rn0jRaFv"
      },
      "source": [
        "## Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JD8q5QoERaFw",
        "outputId": "f2c7bba6-411d-4326-c7ca-6972f3a3ee49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# build BOW features on train reviews\n",
        "cv = CountVectorizer(binary=False, min_df=5, max_df=1.0, ngram_range=(1,2))\n",
        "cv_train_features = cv.fit_transform(norm_train_reviews)\n",
        "\n",
        "\n",
        "# build TFIDF features on train reviews\n",
        "tv = TfidfVectorizer(use_idf=True, min_df=5, max_df=1.0, ngram_range=(1,2),\n",
        "                     sublinear_tf=True)\n",
        "tv_train_features = tv.fit_transform(norm_train_reviews)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 50.9 s, sys: 948 ms, total: 51.9 s\n",
            "Wall time: 52 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sSvqpHRYRaFz",
        "outputId": "3b4e7cdc-6adc-4860-b6df-f252e220860e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "# transform test reviews into features\n",
        "cv_test_features = cv.transform(norm_test_reviews)\n",
        "tv_test_features = tv.transform(norm_test_reviews)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 11.6 s, sys: 12.7 ms, total: 11.6 s\n",
            "Wall time: 11.6 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mfQPYw8PRaF2",
        "outputId": "df84082b-66f6-44e8-a8a0-7dd4b3cd742f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print('BOW model:> Train features shape:', cv_train_features.shape, ' Test features shape:', cv_test_features.shape)\n",
        "print('TFIDF model:> Train features shape:', tv_train_features.shape, ' Test features shape:', tv_test_features.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BOW model:> Train features shape: (35000, 194922)  Test features shape: (15000, 194922)\n",
            "TFIDF model:> Train features shape: (35000, 194922)  Test features shape: (15000, 194922)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3aUUsxMrRaF7"
      },
      "source": [
        "## Model Training, Prediction and Performance Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nPA5UtYFRaF8"
      },
      "source": [
        "### Try out Logistic Regression\n",
        "\n",
        "The logistic regression model is actually a statistical model developed by statistician\n",
        "David Cox in 1958. It is also known as the logit or logistic model since it uses the\n",
        "logistic (popularly also known as sigmoid) mathematical function to estimate the\n",
        "parameter values. These are the coefficients of all our features such that the overall loss\n",
        "is minimized when predicting the outcome—"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JGHQErnMRaF9",
        "outputId": "496db77a-ec1f-451f-f051-ad16bd0a5600",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "# Logistic Regression model on BOW features\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# instantiate model\n",
        "lr = LogisticRegression(penalty='l2', max_iter=500, C=1, solver='lbfgs', random_state=42)\n",
        "\n",
        "# train model\n",
        "lr.fit(cv_train_features, train_sentiments)\n",
        "\n",
        "# predict on test data\n",
        "lr_bow_predictions = lr.predict(cv_test_features)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 47s, sys: 1min 16s, total: 3min 3s\n",
            "Wall time: 1min 33s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wqeRyayrRaGA",
        "outputId": "9478779e-0a6d-44de-e137-21dac53cb923",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "labels = ['negative', 'positive']\n",
        "print(classification_report(test_sentiments, lr_bow_predictions))\n",
        "pd.DataFrame(confusion_matrix(test_sentiments, lr_bow_predictions), index=labels, columns=labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.90      0.90      0.90      7490\n",
            "    positive       0.90      0.91      0.90      7510\n",
            "\n",
            "    accuracy                           0.90     15000\n",
            "   macro avg       0.90      0.90      0.90     15000\n",
            "weighted avg       0.90      0.90      0.90     15000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>negative</th>\n",
              "      <th>positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>negative</th>\n",
              "      <td>6754</td>\n",
              "      <td>736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <td>709</td>\n",
              "      <td>6801</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          negative  positive\n",
              "negative      6754       736\n",
              "positive       709      6801"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uQEAz6O6RaGC",
        "outputId": "a826601c-405d-4271-d5cf-cd2d8d24466c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "# Logistic Regression model on TF-IDF features\n",
        "\n",
        "# train model\n",
        "lr.fit(tv_train_features, train_sentiments)\n",
        "\n",
        "# predict on test data\n",
        "lr_tfidf_predictions = lr.predict(tv_test_features)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 5.07 s, sys: 3.44 s, total: 8.52 s\n",
            "Wall time: 4.44 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HNCfZnUORaGE",
        "outputId": "2e165c81-6794-437b-f065-3266d0b8f8b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "labels = ['negative', 'positive']\n",
        "print(classification_report(test_sentiments, lr_tfidf_predictions))\n",
        "pd.DataFrame(confusion_matrix(test_sentiments, lr_tfidf_predictions), index=labels, columns=labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.91      0.89      0.90      7490\n",
            "    positive       0.90      0.91      0.90      7510\n",
            "\n",
            "    accuracy                           0.90     15000\n",
            "   macro avg       0.90      0.90      0.90     15000\n",
            "weighted avg       0.90      0.90      0.90     15000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>negative</th>\n",
              "      <th>positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>negative</th>\n",
              "      <td>6694</td>\n",
              "      <td>796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <td>665</td>\n",
              "      <td>6845</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          negative  positive\n",
              "negative      6694       796\n",
              "positive       665      6845"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FoOoEAiXRaGH"
      },
      "source": [
        "### Try out Random Forest\n",
        "\n",
        "Decision trees are a family of supervised machine learning algorithms that can represent\n",
        "and interpret sets of rules automatically from the underlying data. They use metrics like\n",
        "information gain and gini-index to build the tree. However, a major drawback of decision\n",
        "trees is that since they are non-parametric, the more data there is, greater the depth of\n",
        "the tree. We can end up with really huge and deep trees that are prone to overfitting. The\n",
        "model might work really well on training data, but instead of learning, it just memorizes\n",
        "all the training samples and builds very specific rules to them. Hence, it performs really\n",
        "poorly on the test data. Random forests try to tackle this problem.\n",
        "\n",
        "A random forest is a meta-estimator or an ensemble model that fits a number of\n",
        "decision tree classifiers on various sub-samples of the dataset and uses averaging to\n",
        "improve the predictive accuracy and control over-fitting. The sub-sample size is always\n",
        "the same as the original input sample size, but the samples are drawn with replacement\n",
        "(bootstrap samples). In random forests, all the trees are trained in parallel (bagging\n",
        "model/bootstrap aggregation). Besides this, each tree in the ensemble is built from a\n",
        "sample drawn with replacement (i.e., a bootstrap sample) from the training set. Also,\n",
        "when splitting a node during the construction of the tree, the split that is chosen is no\n",
        "longer the best split among all features. Instead, the split that is picked is the best split\n",
        "among a random subset of the features. T"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YzaSSOpYRaGH",
        "outputId": "1522d44a-ea57-4e50-8bb3-d72b5eaa49c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "# Random Forest model on BOW features\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# instantiate model\n",
        "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)\n",
        "\n",
        "# train model\n",
        "rf.fit(cv_train_features, train_sentiments)\n",
        "\n",
        "# predict on test data\n",
        "rf_bow_predictions = rf.predict(cv_test_features)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3min 55s, sys: 191 ms, total: 3min 56s\n",
            "Wall time: 2min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "617Kuv7_RaGJ",
        "outputId": "21ee4fed-6d58-4d76-ba55-98ce69f505be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "labels = ['negative', 'positive']\n",
        "print(classification_report(test_sentiments, rf_bow_predictions))\n",
        "pd.DataFrame(confusion_matrix(test_sentiments, rf_bow_predictions), index=labels, columns=labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.86      0.86      0.86      7490\n",
            "    positive       0.86      0.86      0.86      7510\n",
            "\n",
            "    accuracy                           0.86     15000\n",
            "   macro avg       0.86      0.86      0.86     15000\n",
            "weighted avg       0.86      0.86      0.86     15000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>negative</th>\n",
              "      <th>positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>negative</th>\n",
              "      <td>6409</td>\n",
              "      <td>1081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <td>1083</td>\n",
              "      <td>6427</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          negative  positive\n",
              "negative      6409      1081\n",
              "positive      1083      6427"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WdvmBOrPRaGM",
        "outputId": "b7141e34-67af-4466-c7b6-715964803fa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "# Random Forest model on TF-IDF features\n",
        "\n",
        "# train model\n",
        "rf.fit(tv_train_features, train_sentiments)\n",
        "\n",
        "# predict on test data\n",
        "rf_tfidf_predictions = rf.predict(tv_test_features)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3min 27s, sys: 157 ms, total: 3min 27s\n",
            "Wall time: 1min 45s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8hBOMh6uRaGP",
        "outputId": "2b3ff077-fdd9-4e6f-c15c-45d05d6352bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "labels = ['negative', 'positive']\n",
        "print(classification_report(test_sentiments, rf_tfidf_predictions))\n",
        "pd.DataFrame(confusion_matrix(test_sentiments, rf_tfidf_predictions), index=labels, columns=labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.85      0.86      0.85      7490\n",
            "    positive       0.86      0.84      0.85      7510\n",
            "\n",
            "    accuracy                           0.85     15000\n",
            "   macro avg       0.85      0.85      0.85     15000\n",
            "weighted avg       0.85      0.85      0.85     15000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>negative</th>\n",
              "      <th>positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>negative</th>\n",
              "      <td>6447</td>\n",
              "      <td>1043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <td>1174</td>\n",
              "      <td>6336</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          negative  positive\n",
              "negative      6447      1043\n",
              "positive      1174      6336"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QoqZhMQFRaGS"
      },
      "source": [
        "# Newer Supervised Deep Learning Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QZw6LYNHRaGT",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dropout, Activation, Dense\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GiZbcv_gRaGZ"
      },
      "source": [
        "## Prediction class label encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JnhC4rWaRaGb",
        "colab": {}
      },
      "source": [
        "le = LabelEncoder()\n",
        "# tokenize train reviews & encode train labels\n",
        "tokenized_train = [nltk.word_tokenize(text)\n",
        "                       for text in norm_train_reviews]\n",
        "y_train = le.fit_transform(train_sentiments)\n",
        "# tokenize test reviews & encode test labels\n",
        "tokenized_test = [nltk.word_tokenize(text)\n",
        "                       for text in norm_test_reviews]\n",
        "y_test = le.fit_transform(test_sentiments)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4ogDRDh4RaGg",
        "outputId": "b8dab754-79f8-4fe3-a1c6-deb086ed94ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "# print class label encoding map and encoded labels\n",
        "print('Sentiment class label map:', dict(zip(le.classes_, le.transform(le.classes_))))\n",
        "print('Sample test label transformation:\\n'+'-'*35,\n",
        "      '\\nActual Labels:', test_sentiments[:3], '\\nEncoded Labels:', y_test[:3])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentiment class label map: {'negative': 0, 'positive': 1}\n",
            "Sample test label transformation:\n",
            "----------------------------------- \n",
            "Actual Labels: ['negative' 'positive' 'negative'] \n",
            "Encoded Labels: [0 1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hdexbrYXRaGk"
      },
      "source": [
        "## Feature Engineering with word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G5S0u0BbiN2a",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T9kfCw6LRaGl",
        "outputId": "b9851c01-38d2-4e6c-d332-0ed3d9ae0ccf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "# build word2vec model\n",
        "w2v_num_features = 300\n",
        "w2v_model = gensim.models.Word2Vec(tokenized_train, size=w2v_num_features, window=150,\n",
        "                                   min_count=10, workers=4, iter=5)    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-09-15 03:44:51,439 : INFO : collecting all words and their counts\n",
            "2019-09-15 03:44:51,442 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2019-09-15 03:44:51,932 : INFO : PROGRESS: at sentence #10000, processed 2294933 words, keeping 82417 word types\n",
            "2019-09-15 03:44:52,437 : INFO : PROGRESS: at sentence #20000, processed 4591079 words, keeping 124832 word types\n",
            "2019-09-15 03:44:52,929 : INFO : PROGRESS: at sentence #30000, processed 6884452 words, keeping 159825 word types\n",
            "2019-09-15 03:44:53,195 : INFO : collected 176258 word types from a corpus of 8035381 raw words and 35000 sentences\n",
            "2019-09-15 03:44:53,197 : INFO : Loading a fresh vocabulary\n",
            "2019-09-15 03:44:53,315 : INFO : effective_min_count=10 retains 24661 unique words (13% of original 176258, drops 151597)\n",
            "2019-09-15 03:44:53,317 : INFO : effective_min_count=10 leaves 7763119 word corpus (96% of original 8035381, drops 272262)\n",
            "2019-09-15 03:44:53,412 : INFO : deleting the raw counts dictionary of 176258 items\n",
            "2019-09-15 03:44:53,420 : INFO : sample=0.001 downsamples 49 most-common words\n",
            "2019-09-15 03:44:53,421 : INFO : downsampling leaves estimated 5721143 word corpus (73.7% of prior 7763119)\n",
            "2019-09-15 03:44:53,486 : INFO : estimated required memory for 24661 words and 300 dimensions: 71516900 bytes\n",
            "2019-09-15 03:44:53,487 : INFO : resetting layer weights\n",
            "2019-09-15 03:44:53,825 : INFO : training model with 4 workers on 24661 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=150\n",
            "2019-09-15 03:44:55,176 : INFO : EPOCH 1 - PROGRESS: at 1.11% examples, 47117 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:44:56,254 : INFO : EPOCH 1 - PROGRESS: at 2.42% examples, 58105 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:44:57,398 : INFO : EPOCH 1 - PROGRESS: at 3.79% examples, 59089 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:44:58,484 : INFO : EPOCH 1 - PROGRESS: at 4.99% examples, 60180 words/s, in_qsize 8, out_qsize 0\n",
            "2019-09-15 03:44:59,633 : INFO : EPOCH 1 - PROGRESS: at 6.24% examples, 60314 words/s, in_qsize 6, out_qsize 1\n",
            "2019-09-15 03:45:00,781 : INFO : EPOCH 1 - PROGRESS: at 7.45% examples, 60543 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:01,809 : INFO : EPOCH 1 - PROGRESS: at 8.46% examples, 60478 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:02,847 : INFO : EPOCH 1 - PROGRESS: at 9.69% examples, 61262 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:04,014 : INFO : EPOCH 1 - PROGRESS: at 10.81% examples, 60409 words/s, in_qsize 7, out_qsize 1\n",
            "2019-09-15 03:45:05,119 : INFO : EPOCH 1 - PROGRESS: at 12.20% examples, 61332 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:06,277 : INFO : EPOCH 1 - PROGRESS: at 13.27% examples, 60677 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:07,314 : INFO : EPOCH 1 - PROGRESS: at 14.43% examples, 61170 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:08,603 : INFO : EPOCH 1 - PROGRESS: at 15.55% examples, 60503 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:09,674 : INFO : EPOCH 1 - PROGRESS: at 16.71% examples, 60861 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:10,885 : INFO : EPOCH 1 - PROGRESS: at 17.90% examples, 60647 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:11,895 : INFO : EPOCH 1 - PROGRESS: at 19.07% examples, 60753 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:12,906 : INFO : EPOCH 1 - PROGRESS: at 20.29% examples, 61237 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:13,965 : INFO : EPOCH 1 - PROGRESS: at 21.42% examples, 61141 words/s, in_qsize 8, out_qsize 0\n",
            "2019-09-15 03:45:15,050 : INFO : EPOCH 1 - PROGRESS: at 22.53% examples, 60958 words/s, in_qsize 6, out_qsize 1\n",
            "2019-09-15 03:45:16,086 : INFO : EPOCH 1 - PROGRESS: at 23.76% examples, 61259 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:17,363 : INFO : EPOCH 1 - PROGRESS: at 24.98% examples, 60915 words/s, in_qsize 8, out_qsize 0\n",
            "2019-09-15 03:45:18,694 : INFO : EPOCH 1 - PROGRESS: at 26.38% examples, 60980 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:20,047 : INFO : EPOCH 1 - PROGRESS: at 27.95% examples, 61060 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:21,065 : INFO : EPOCH 1 - PROGRESS: at 29.16% examples, 61321 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:22,203 : INFO : EPOCH 1 - PROGRESS: at 30.20% examples, 61076 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:23,304 : INFO : EPOCH 1 - PROGRESS: at 31.48% examples, 61377 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:24,503 : INFO : EPOCH 1 - PROGRESS: at 32.67% examples, 61239 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:25,626 : INFO : EPOCH 1 - PROGRESS: at 33.89% examples, 61266 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:26,744 : INFO : EPOCH 1 - PROGRESS: at 35.18% examples, 61301 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:27,761 : INFO : EPOCH 1 - PROGRESS: at 36.41% examples, 61526 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:28,854 : INFO : EPOCH 1 - PROGRESS: at 37.52% examples, 61409 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:29,947 : INFO : EPOCH 1 - PROGRESS: at 38.73% examples, 61509 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:31,077 : INFO : EPOCH 1 - PROGRESS: at 39.80% examples, 61318 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:32,085 : INFO : EPOCH 1 - PROGRESS: at 40.90% examples, 61335 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:33,146 : INFO : EPOCH 1 - PROGRESS: at 42.18% examples, 61473 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:34,157 : INFO : EPOCH 1 - PROGRESS: at 43.28% examples, 61494 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:35,239 : INFO : EPOCH 1 - PROGRESS: at 44.35% examples, 61408 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:36,318 : INFO : EPOCH 1 - PROGRESS: at 45.49% examples, 61327 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:37,331 : INFO : EPOCH 1 - PROGRESS: at 46.59% examples, 61359 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:38,437 : INFO : EPOCH 1 - PROGRESS: at 47.71% examples, 61270 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:39,439 : INFO : EPOCH 1 - PROGRESS: at 48.93% examples, 61453 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:40,679 : INFO : EPOCH 1 - PROGRESS: at 50.20% examples, 61317 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:41,756 : INFO : EPOCH 1 - PROGRESS: at 51.34% examples, 61262 words/s, in_qsize 8, out_qsize 1\n",
            "2019-09-15 03:45:42,842 : INFO : EPOCH 1 - PROGRESS: at 52.43% examples, 61201 words/s, in_qsize 6, out_qsize 1\n",
            "2019-09-15 03:45:43,890 : INFO : EPOCH 1 - PROGRESS: at 53.63% examples, 61321 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:45,108 : INFO : EPOCH 1 - PROGRESS: at 54.92% examples, 61224 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:46,142 : INFO : EPOCH 1 - PROGRESS: at 56.26% examples, 61490 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:47,407 : INFO : EPOCH 1 - PROGRESS: at 57.31% examples, 61207 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:48,442 : INFO : EPOCH 1 - PROGRESS: at 58.50% examples, 61327 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:49,746 : INFO : EPOCH 1 - PROGRESS: at 59.68% examples, 61154 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:50,787 : INFO : EPOCH 1 - PROGRESS: at 61.03% examples, 61390 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:52,094 : INFO : EPOCH 1 - PROGRESS: at 62.16% examples, 61088 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:53,497 : INFO : EPOCH 1 - PROGRESS: at 63.62% examples, 61056 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:54,513 : INFO : EPOCH 1 - PROGRESS: at 64.92% examples, 61304 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:55,593 : INFO : EPOCH 1 - PROGRESS: at 65.89% examples, 61135 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:56,645 : INFO : EPOCH 1 - PROGRESS: at 67.13% examples, 61237 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:57,660 : INFO : EPOCH 1 - PROGRESS: at 68.25% examples, 61252 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:58,875 : INFO : EPOCH 1 - PROGRESS: at 69.36% examples, 61070 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:45:59,912 : INFO : EPOCH 1 - PROGRESS: at 70.70% examples, 61282 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:01,200 : INFO : EPOCH 1 - PROGRESS: at 71.81% examples, 61047 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:02,590 : INFO : EPOCH 1 - PROGRESS: at 73.27% examples, 61041 words/s, in_qsize 6, out_qsize 1\n",
            "2019-09-15 03:46:03,943 : INFO : EPOCH 1 - PROGRESS: at 74.72% examples, 61054 words/s, in_qsize 7, out_qsize 1\n",
            "2019-09-15 03:46:05,265 : INFO : EPOCH 1 - PROGRESS: at 76.24% examples, 61087 words/s, in_qsize 7, out_qsize 1\n",
            "2019-09-15 03:46:06,598 : INFO : EPOCH 1 - PROGRESS: at 77.78% examples, 61127 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:07,940 : INFO : EPOCH 1 - PROGRESS: at 79.20% examples, 61154 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:08,942 : INFO : EPOCH 1 - PROGRESS: at 80.37% examples, 61271 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:10,185 : INFO : EPOCH 1 - PROGRESS: at 81.67% examples, 61188 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:11,262 : INFO : EPOCH 1 - PROGRESS: at 82.93% examples, 61239 words/s, in_qsize 8, out_qsize 1\n",
            "2019-09-15 03:46:12,324 : INFO : EPOCH 1 - PROGRESS: at 84.31% examples, 61392 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:13,535 : INFO : EPOCH 1 - PROGRESS: at 85.37% examples, 61268 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:14,689 : INFO : EPOCH 1 - PROGRESS: at 86.69% examples, 61337 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:15,811 : INFO : EPOCH 1 - PROGRESS: at 87.75% examples, 61271 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:16,818 : INFO : EPOCH 1 - PROGRESS: at 89.17% examples, 61454 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:18,000 : INFO : EPOCH 1 - PROGRESS: at 90.18% examples, 61346 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:19,094 : INFO : EPOCH 1 - PROGRESS: at 91.25% examples, 61293 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:20,305 : INFO : EPOCH 1 - PROGRESS: at 92.58% examples, 61321 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:21,366 : INFO : EPOCH 1 - PROGRESS: at 93.83% examples, 61377 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:22,464 : INFO : EPOCH 1 - PROGRESS: at 95.12% examples, 61412 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:23,495 : INFO : EPOCH 1 - PROGRESS: at 96.27% examples, 61419 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:24,503 : INFO : EPOCH 1 - PROGRESS: at 97.53% examples, 61496 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:25,601 : INFO : EPOCH 1 - PROGRESS: at 98.57% examples, 61442 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:26,644 : INFO : EPOCH 1 - PROGRESS: at 99.64% examples, 61429 words/s, in_qsize 3, out_qsize 0\n",
            "2019-09-15 03:46:26,651 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-09-15 03:46:26,712 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-09-15 03:46:26,795 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-09-15 03:46:26,803 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-09-15 03:46:26,804 : INFO : EPOCH - 1 : training on 8035381 raw words (5720846 effective words) took 93.0s, 61534 effective words/s\n",
            "2019-09-15 03:46:28,144 : INFO : EPOCH 2 - PROGRESS: at 1.11% examples, 47686 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:29,160 : INFO : EPOCH 2 - PROGRESS: at 2.29% examples, 57051 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:30,183 : INFO : EPOCH 2 - PROGRESS: at 3.63% examples, 60578 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:31,271 : INFO : EPOCH 2 - PROGRESS: at 4.73% examples, 59710 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:32,382 : INFO : EPOCH 2 - PROGRESS: at 5.99% examples, 60401 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:33,528 : INFO : EPOCH 2 - PROGRESS: at 7.21% examples, 60489 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:34,668 : INFO : EPOCH 2 - PROGRESS: at 8.36% examples, 60576 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:35,727 : INFO : EPOCH 2 - PROGRESS: at 9.58% examples, 61229 words/s, in_qsize 8, out_qsize 0\n",
            "2019-09-15 03:46:36,749 : INFO : EPOCH 2 - PROGRESS: at 10.69% examples, 61292 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:37,896 : INFO : EPOCH 2 - PROGRESS: at 11.83% examples, 60617 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:38,957 : INFO : EPOCH 2 - PROGRESS: at 12.92% examples, 60456 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:39,969 : INFO : EPOCH 2 - PROGRESS: at 14.04% examples, 61116 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:41,128 : INFO : EPOCH 2 - PROGRESS: at 15.13% examples, 60593 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:42,165 : INFO : EPOCH 2 - PROGRESS: at 16.14% examples, 60531 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:43,183 : INFO : EPOCH 2 - PROGRESS: at 17.12% examples, 60651 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:44,291 : INFO : EPOCH 2 - PROGRESS: at 18.46% examples, 60850 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:45,283 : INFO : EPOCH 2 - PROGRESS: at 19.52% examples, 60936 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:46,396 : INFO : EPOCH 2 - PROGRESS: at 20.65% examples, 60709 words/s, in_qsize 6, out_qsize 1\n",
            "2019-09-15 03:46:47,532 : INFO : EPOCH 2 - PROGRESS: at 21.93% examples, 60777 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:48,641 : INFO : EPOCH 2 - PROGRESS: at 23.17% examples, 60887 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:49,642 : INFO : EPOCH 2 - PROGRESS: at 24.27% examples, 60983 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:50,778 : INFO : EPOCH 2 - PROGRESS: at 25.43% examples, 61036 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:51,927 : INFO : EPOCH 2 - PROGRESS: at 26.51% examples, 60677 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:53,032 : INFO : EPOCH 2 - PROGRESS: at 27.95% examples, 61078 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:54,050 : INFO : EPOCH 2 - PROGRESS: at 29.04% examples, 61087 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:55,144 : INFO : EPOCH 2 - PROGRESS: at 30.06% examples, 60942 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:56,169 : INFO : EPOCH 2 - PROGRESS: at 31.25% examples, 61167 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:57,196 : INFO : EPOCH 2 - PROGRESS: at 32.25% examples, 61154 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:58,343 : INFO : EPOCH 2 - PROGRESS: at 33.40% examples, 60932 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:46:59,454 : INFO : EPOCH 2 - PROGRESS: at 34.80% examples, 61210 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:00,514 : INFO : EPOCH 2 - PROGRESS: at 35.92% examples, 61146 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:01,642 : INFO : EPOCH 2 - PROGRESS: at 37.15% examples, 61178 words/s, in_qsize 7, out_qsize 1\n",
            "2019-09-15 03:47:02,819 : INFO : EPOCH 2 - PROGRESS: at 38.35% examples, 61125 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:04,008 : INFO : EPOCH 2 - PROGRESS: at 39.68% examples, 61242 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:05,209 : INFO : EPOCH 2 - PROGRESS: at 40.90% examples, 61130 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:06,215 : INFO : EPOCH 2 - PROGRESS: at 42.18% examples, 61354 words/s, in_qsize 8, out_qsize 1\n",
            "2019-09-15 03:47:07,513 : INFO : EPOCH 2 - PROGRESS: at 43.40% examples, 61115 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:08,859 : INFO : EPOCH 2 - PROGRESS: at 44.88% examples, 61144 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:09,866 : INFO : EPOCH 2 - PROGRESS: at 46.17% examples, 61512 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:10,873 : INFO : EPOCH 2 - PROGRESS: at 47.23% examples, 61392 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:11,899 : INFO : EPOCH 2 - PROGRESS: at 48.32% examples, 61391 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:13,012 : INFO : EPOCH 2 - PROGRESS: at 49.42% examples, 61273 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:14,115 : INFO : EPOCH 2 - PROGRESS: at 50.71% examples, 61333 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:15,174 : INFO : EPOCH 2 - PROGRESS: at 51.80% examples, 61309 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:16,271 : INFO : EPOCH 2 - PROGRESS: at 52.90% examples, 61218 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:17,279 : INFO : EPOCH 2 - PROGRESS: at 54.16% examples, 61388 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:18,394 : INFO : EPOCH 2 - PROGRESS: at 55.29% examples, 61272 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:19,411 : INFO : EPOCH 2 - PROGRESS: at 56.37% examples, 61292 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:20,425 : INFO : EPOCH 2 - PROGRESS: at 57.54% examples, 61437 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:21,557 : INFO : EPOCH 2 - PROGRESS: at 58.59% examples, 61311 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:22,717 : INFO : EPOCH 2 - PROGRESS: at 59.83% examples, 61292 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:23,767 : INFO : EPOCH 2 - PROGRESS: at 61.03% examples, 61392 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:24,814 : INFO : EPOCH 2 - PROGRESS: at 62.14% examples, 61368 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:25,849 : INFO : EPOCH 2 - PROGRESS: at 63.39% examples, 61480 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:26,874 : INFO : EPOCH 2 - PROGRESS: at 64.44% examples, 61474 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:28,069 : INFO : EPOCH 2 - PROGRESS: at 65.51% examples, 61300 words/s, in_qsize 8, out_qsize 0\n",
            "2019-09-15 03:47:29,100 : INFO : EPOCH 2 - PROGRESS: at 66.75% examples, 61414 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:30,115 : INFO : EPOCH 2 - PROGRESS: at 67.86% examples, 61426 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:31,230 : INFO : EPOCH 2 - PROGRESS: at 69.01% examples, 61342 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:32,256 : INFO : EPOCH 2 - PROGRESS: at 70.10% examples, 61347 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:33,265 : INFO : EPOCH 2 - PROGRESS: at 71.21% examples, 61362 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:34,459 : INFO : EPOCH 2 - PROGRESS: at 72.57% examples, 61421 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:35,461 : INFO : EPOCH 2 - PROGRESS: at 73.57% examples, 61442 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:36,670 : INFO : EPOCH 2 - PROGRESS: at 74.99% examples, 61475 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:37,670 : INFO : EPOCH 2 - PROGRESS: at 76.11% examples, 61483 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:38,882 : INFO : EPOCH 2 - PROGRESS: at 77.51% examples, 61521 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:39,959 : INFO : EPOCH 2 - PROGRESS: at 78.60% examples, 61488 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:40,975 : INFO : EPOCH 2 - PROGRESS: at 79.68% examples, 61491 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:42,077 : INFO : EPOCH 2 - PROGRESS: at 80.90% examples, 61514 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:43,115 : INFO : EPOCH 2 - PROGRESS: at 82.02% examples, 61502 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:44,422 : INFO : EPOCH 2 - PROGRESS: at 83.47% examples, 61456 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:45,478 : INFO : EPOCH 2 - PROGRESS: at 84.57% examples, 61439 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:46,745 : INFO : EPOCH 2 - PROGRESS: at 85.89% examples, 61438 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:47,771 : INFO : EPOCH 2 - PROGRESS: at 86.92% examples, 61434 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:48,775 : INFO : EPOCH 2 - PROGRESS: at 88.14% examples, 61544 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:50,024 : INFO : EPOCH 2 - PROGRESS: at 89.27% examples, 61377 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:51,034 : INFO : EPOCH 2 - PROGRESS: at 90.30% examples, 61393 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:52,343 : INFO : EPOCH 2 - PROGRESS: at 91.61% examples, 61357 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:53,334 : INFO : EPOCH 2 - PROGRESS: at 92.85% examples, 61450 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:54,644 : INFO : EPOCH 2 - PROGRESS: at 94.10% examples, 61335 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:55,663 : INFO : EPOCH 2 - PROGRESS: at 95.49% examples, 61502 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:56,987 : INFO : EPOCH 2 - PROGRESS: at 96.79% examples, 61373 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:58,096 : INFO : EPOCH 2 - PROGRESS: at 98.10% examples, 61466 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:59,204 : INFO : EPOCH 2 - PROGRESS: at 99.12% examples, 61406 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:47:59,640 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-09-15 03:47:59,704 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-09-15 03:47:59,749 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-09-15 03:47:59,760 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-09-15 03:47:59,761 : INFO : EPOCH - 2 : training on 8035381 raw words (5721276 effective words) took 92.9s, 61553 effective words/s\n",
            "2019-09-15 03:48:00,783 : INFO : EPOCH 3 - PROGRESS: at 0.87% examples, 48258 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:02,055 : INFO : EPOCH 3 - PROGRESS: at 2.01% examples, 52229 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:03,079 : INFO : EPOCH 3 - PROGRESS: at 3.49% examples, 59473 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:04,312 : INFO : EPOCH 3 - PROGRESS: at 4.62% examples, 57001 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:05,373 : INFO : EPOCH 3 - PROGRESS: at 5.99% examples, 59995 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:06,563 : INFO : EPOCH 3 - PROGRESS: at 7.09% examples, 58719 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:07,636 : INFO : EPOCH 3 - PROGRESS: at 8.24% examples, 59580 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:08,844 : INFO : EPOCH 3 - PROGRESS: at 9.46% examples, 59361 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:09,917 : INFO : EPOCH 3 - PROGRESS: at 10.69% examples, 59996 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:11,040 : INFO : EPOCH 3 - PROGRESS: at 11.95% examples, 60229 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:12,073 : INFO : EPOCH 3 - PROGRESS: at 13.04% examples, 60257 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:13,178 : INFO : EPOCH 3 - PROGRESS: at 14.17% examples, 60508 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:14,187 : INFO : EPOCH 3 - PROGRESS: at 15.24% examples, 60614 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:15,317 : INFO : EPOCH 3 - PROGRESS: at 16.25% examples, 60219 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:16,322 : INFO : EPOCH 3 - PROGRESS: at 17.23% examples, 60383 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:17,364 : INFO : EPOCH 3 - PROGRESS: at 18.57% examples, 60783 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:18,523 : INFO : EPOCH 3 - PROGRESS: at 19.65% examples, 60384 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:19,614 : INFO : EPOCH 3 - PROGRESS: at 21.04% examples, 60977 words/s, in_qsize 8, out_qsize 0\n",
            "2019-09-15 03:48:20,735 : INFO : EPOCH 3 - PROGRESS: at 22.18% examples, 60734 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:21,735 : INFO : EPOCH 3 - PROGRESS: at 23.17% examples, 60495 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:22,786 : INFO : EPOCH 3 - PROGRESS: at 24.39% examples, 60797 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:23,915 : INFO : EPOCH 3 - PROGRESS: at 25.57% examples, 60809 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:24,933 : INFO : EPOCH 3 - PROGRESS: at 26.62% examples, 60821 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:26,018 : INFO : EPOCH 3 - PROGRESS: at 27.81% examples, 60719 words/s, in_qsize 7, out_qsize 1\n",
            "2019-09-15 03:48:27,118 : INFO : EPOCH 3 - PROGRESS: at 29.16% examples, 61070 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:28,298 : INFO : EPOCH 3 - PROGRESS: at 30.20% examples, 60751 words/s, in_qsize 8, out_qsize 0\n",
            "2019-09-15 03:48:29,441 : INFO : EPOCH 3 - PROGRESS: at 31.48% examples, 60972 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:30,562 : INFO : EPOCH 3 - PROGRESS: at 32.56% examples, 60779 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:31,610 : INFO : EPOCH 3 - PROGRESS: at 33.78% examples, 60969 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:32,960 : INFO : EPOCH 3 - PROGRESS: at 35.18% examples, 60804 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:34,014 : INFO : EPOCH 3 - PROGRESS: at 36.41% examples, 60973 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:35,038 : INFO : EPOCH 3 - PROGRESS: at 37.65% examples, 61198 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:36,090 : INFO : EPOCH 3 - PROGRESS: at 38.73% examples, 61157 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:37,111 : INFO : EPOCH 3 - PROGRESS: at 39.68% examples, 60985 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:38,199 : INFO : EPOCH 3 - PROGRESS: at 41.03% examples, 61242 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:39,350 : INFO : EPOCH 3 - PROGRESS: at 42.18% examples, 61070 words/s, in_qsize 6, out_qsize 1\n",
            "2019-09-15 03:48:40,357 : INFO : EPOCH 3 - PROGRESS: at 43.40% examples, 61278 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:41,591 : INFO : EPOCH 3 - PROGRESS: at 44.62% examples, 61133 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:42,620 : INFO : EPOCH 3 - PROGRESS: at 45.83% examples, 61303 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:43,798 : INFO : EPOCH 3 - PROGRESS: at 47.11% examples, 61272 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:44,919 : INFO : EPOCH 3 - PROGRESS: at 48.19% examples, 61159 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:45,962 : INFO : EPOCH 3 - PROGRESS: at 49.55% examples, 61435 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:47,167 : INFO : EPOCH 3 - PROGRESS: at 50.71% examples, 61214 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:48,176 : INFO : EPOCH 3 - PROGRESS: at 52.05% examples, 61538 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:49,413 : INFO : EPOCH 3 - PROGRESS: at 53.13% examples, 61275 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:50,677 : INFO : EPOCH 3 - PROGRESS: at 54.71% examples, 61409 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:51,692 : INFO : EPOCH 3 - PROGRESS: at 55.82% examples, 61415 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:53,027 : INFO : EPOCH 3 - PROGRESS: at 57.08% examples, 61327 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:54,098 : INFO : EPOCH 3 - PROGRESS: at 58.38% examples, 61522 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:55,261 : INFO : EPOCH 3 - PROGRESS: at 59.41% examples, 61376 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:56,415 : INFO : EPOCH 3 - PROGRESS: at 60.79% examples, 61485 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:57,509 : INFO : EPOCH 3 - PROGRESS: at 61.90% examples, 61413 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:58,544 : INFO : EPOCH 3 - PROGRESS: at 63.15% examples, 61534 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:48:59,559 : INFO : EPOCH 3 - PROGRESS: at 64.21% examples, 61545 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:00,693 : INFO : EPOCH 3 - PROGRESS: at 65.27% examples, 61409 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:01,805 : INFO : EPOCH 3 - PROGRESS: at 66.62% examples, 61552 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:02,902 : INFO : EPOCH 3 - PROGRESS: at 67.74% examples, 61489 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:03,912 : INFO : EPOCH 3 - PROGRESS: at 68.90% examples, 61508 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:04,999 : INFO : EPOCH 3 - PROGRESS: at 70.10% examples, 61557 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:06,007 : INFO : EPOCH 3 - PROGRESS: at 71.22% examples, 61569 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:07,129 : INFO : EPOCH 3 - PROGRESS: at 72.32% examples, 61482 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:08,209 : INFO : EPOCH 3 - PROGRESS: at 73.47% examples, 61538 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:09,391 : INFO : EPOCH 3 - PROGRESS: at 74.71% examples, 61492 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:10,492 : INFO : EPOCH 3 - PROGRESS: at 76.11% examples, 61610 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:11,634 : INFO : EPOCH 3 - PROGRESS: at 77.27% examples, 61509 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:12,680 : INFO : EPOCH 3 - PROGRESS: at 78.60% examples, 61683 words/s, in_qsize 8, out_qsize 0\n",
            "2019-09-15 03:49:13,857 : INFO : EPOCH 3 - PROGRESS: at 79.68% examples, 61562 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:14,868 : INFO : EPOCH 3 - PROGRESS: at 80.90% examples, 61656 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:15,997 : INFO : EPOCH 3 - PROGRESS: at 82.14% examples, 61664 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:17,103 : INFO : EPOCH 3 - PROGRESS: at 83.47% examples, 61695 words/s, in_qsize 8, out_qsize 0\n",
            "2019-09-15 03:49:18,126 : INFO : EPOCH 3 - PROGRESS: at 84.57% examples, 61691 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:19,232 : INFO : EPOCH 3 - PROGRESS: at 85.75% examples, 61723 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:20,324 : INFO : EPOCH 3 - PROGRESS: at 86.81% examples, 61669 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:21,481 : INFO : EPOCH 3 - PROGRESS: at 88.14% examples, 61745 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:22,579 : INFO : EPOCH 3 - PROGRESS: at 89.29% examples, 61684 words/s, in_qsize 6, out_qsize 1\n",
            "2019-09-15 03:49:23,613 : INFO : EPOCH 3 - PROGRESS: at 90.41% examples, 61763 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:24,884 : INFO : EPOCH 3 - PROGRESS: at 91.61% examples, 61655 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:25,899 : INFO : EPOCH 3 - PROGRESS: at 92.85% examples, 61737 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:26,918 : INFO : EPOCH 3 - PROGRESS: at 94.10% examples, 61822 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:27,982 : INFO : EPOCH 3 - PROGRESS: at 95.24% examples, 61793 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:28,988 : INFO : EPOCH 3 - PROGRESS: at 96.39% examples, 61807 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:30,243 : INFO : EPOCH 3 - PROGRESS: at 97.76% examples, 61798 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:31,268 : INFO : EPOCH 3 - PROGRESS: at 98.87% examples, 61859 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:32,011 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-09-15 03:49:32,070 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-09-15 03:49:32,076 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-09-15 03:49:32,098 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-09-15 03:49:32,099 : INFO : EPOCH - 3 : training on 8035381 raw words (5722324 effective words) took 92.3s, 61975 effective words/s\n",
            "2019-09-15 03:49:33,447 : INFO : EPOCH 4 - PROGRESS: at 1.07% examples, 47025 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:34,782 : INFO : EPOCH 4 - PROGRESS: at 2.57% examples, 55111 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:36,100 : INFO : EPOCH 4 - PROGRESS: at 4.18% examples, 58022 words/s, in_qsize 6, out_qsize 1\n",
            "2019-09-15 03:49:37,446 : INFO : EPOCH 4 - PROGRESS: at 5.63% examples, 59015 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:38,789 : INFO : EPOCH 4 - PROGRESS: at 7.09% examples, 59658 words/s, in_qsize 6, out_qsize 1\n",
            "2019-09-15 03:49:40,140 : INFO : EPOCH 4 - PROGRESS: at 8.46% examples, 60022 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:41,457 : INFO : EPOCH 4 - PROGRESS: at 9.95% examples, 60526 words/s, in_qsize 6, out_qsize 1\n",
            "2019-09-15 03:49:42,684 : INFO : EPOCH 4 - PROGRESS: at 11.42% examples, 61454 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:43,704 : INFO : EPOCH 4 - PROGRESS: at 12.56% examples, 61472 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:44,743 : INFO : EPOCH 4 - PROGRESS: at 13.69% examples, 61927 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:45,772 : INFO : EPOCH 4 - PROGRESS: at 14.77% examples, 61899 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:46,843 : INFO : EPOCH 4 - PROGRESS: at 15.77% examples, 61575 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:47,955 : INFO : EPOCH 4 - PROGRESS: at 16.91% examples, 61709 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:49,126 : INFO : EPOCH 4 - PROGRESS: at 18.19% examples, 61628 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:50,165 : INFO : EPOCH 4 - PROGRESS: at 19.52% examples, 62282 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:51,362 : INFO : EPOCH 4 - PROGRESS: at 20.65% examples, 61694 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:52,364 : INFO : EPOCH 4 - PROGRESS: at 22.05% examples, 62469 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:53,704 : INFO : EPOCH 4 - PROGRESS: at 23.29% examples, 61812 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:55,033 : INFO : EPOCH 4 - PROGRESS: at 24.74% examples, 61888 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:56,034 : INFO : EPOCH 4 - PROGRESS: at 25.89% examples, 62175 words/s, in_qsize 6, out_qsize 1\n",
            "2019-09-15 03:49:57,241 : INFO : EPOCH 4 - PROGRESS: at 27.15% examples, 61981 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:58,251 : INFO : EPOCH 4 - PROGRESS: at 28.47% examples, 62277 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:49:59,316 : INFO : EPOCH 4 - PROGRESS: at 29.60% examples, 62366 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:00,437 : INFO : EPOCH 4 - PROGRESS: at 30.66% examples, 62120 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:01,572 : INFO : EPOCH 4 - PROGRESS: at 31.94% examples, 62291 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:02,684 : INFO : EPOCH 4 - PROGRESS: at 33.05% examples, 62087 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:03,824 : INFO : EPOCH 4 - PROGRESS: at 34.38% examples, 62275 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:05,010 : INFO : EPOCH 4 - PROGRESS: at 35.71% examples, 62143 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:06,086 : INFO : EPOCH 4 - PROGRESS: at 37.02% examples, 62444 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:07,276 : INFO : EPOCH 4 - PROGRESS: at 38.10% examples, 62127 words/s, in_qsize 6, out_qsize 1\n",
            "2019-09-15 03:50:08,354 : INFO : EPOCH 4 - PROGRESS: at 39.42% examples, 62405 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:09,424 : INFO : EPOCH 4 - PROGRESS: at 40.55% examples, 62287 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:10,425 : INFO : EPOCH 4 - PROGRESS: at 41.69% examples, 62309 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:11,561 : INFO : EPOCH 4 - PROGRESS: at 42.89% examples, 62295 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:12,599 : INFO : EPOCH 4 - PROGRESS: at 44.13% examples, 62438 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:13,778 : INFO : EPOCH 4 - PROGRESS: at 45.37% examples, 62335 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:14,848 : INFO : EPOCH 4 - PROGRESS: at 46.59% examples, 62424 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:15,895 : INFO : EPOCH 4 - PROGRESS: at 47.71% examples, 62391 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:16,903 : INFO : EPOCH 4 - PROGRESS: at 48.82% examples, 62382 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:18,075 : INFO : EPOCH 4 - PROGRESS: at 50.20% examples, 62462 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:19,076 : INFO : EPOCH 4 - PROGRESS: at 51.32% examples, 62482 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:20,131 : INFO : EPOCH 4 - PROGRESS: at 52.57% examples, 62575 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:21,241 : INFO : EPOCH 4 - PROGRESS: at 53.77% examples, 62591 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:22,329 : INFO : EPOCH 4 - PROGRESS: at 55.06% examples, 62622 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:23,473 : INFO : EPOCH 4 - PROGRESS: at 56.26% examples, 62597 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:24,597 : INFO : EPOCH 4 - PROGRESS: at 57.41% examples, 62584 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:25,646 : INFO : EPOCH 4 - PROGRESS: at 58.59% examples, 62659 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:26,839 : INFO : EPOCH 4 - PROGRESS: at 59.83% examples, 62579 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:27,967 : INFO : EPOCH 4 - PROGRESS: at 61.14% examples, 62692 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:29,021 : INFO : EPOCH 4 - PROGRESS: at 62.26% examples, 62637 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:30,221 : INFO : EPOCH 4 - PROGRESS: at 63.62% examples, 62666 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:31,321 : INFO : EPOCH 4 - PROGRESS: at 64.81% examples, 62682 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:32,491 : INFO : EPOCH 4 - PROGRESS: at 66.01% examples, 62622 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:33,593 : INFO : EPOCH 4 - PROGRESS: at 67.37% examples, 62760 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:34,676 : INFO : EPOCH 4 - PROGRESS: at 68.49% examples, 62680 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:35,743 : INFO : EPOCH 4 - PROGRESS: at 69.87% examples, 62838 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:36,871 : INFO : EPOCH 4 - PROGRESS: at 70.95% examples, 62716 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:38,034 : INFO : EPOCH 4 - PROGRESS: at 72.32% examples, 62783 words/s, in_qsize 8, out_qsize 0\n",
            "2019-09-15 03:50:39,107 : INFO : EPOCH 4 - PROGRESS: at 73.37% examples, 62721 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:40,276 : INFO : EPOCH 4 - PROGRESS: at 74.71% examples, 62765 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:41,330 : INFO : EPOCH 4 - PROGRESS: at 76.11% examples, 62907 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:42,450 : INFO : EPOCH 4 - PROGRESS: at 77.27% examples, 62802 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:43,475 : INFO : EPOCH 4 - PROGRESS: at 78.46% examples, 62882 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:44,694 : INFO : EPOCH 4 - PROGRESS: at 79.68% examples, 62799 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:45,939 : INFO : EPOCH 4 - PROGRESS: at 81.19% examples, 62869 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:47,065 : INFO : EPOCH 4 - PROGRESS: at 82.38% examples, 62856 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:48,194 : INFO : EPOCH 4 - PROGRESS: at 83.85% examples, 62934 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:49,288 : INFO : EPOCH 4 - PROGRESS: at 84.89% examples, 62866 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:50,297 : INFO : EPOCH 4 - PROGRESS: at 86.11% examples, 62960 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:51,317 : INFO : EPOCH 4 - PROGRESS: at 87.13% examples, 62944 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:52,423 : INFO : EPOCH 4 - PROGRESS: at 88.26% examples, 62868 words/s, in_qsize 8, out_qsize 0\n",
            "2019-09-15 03:50:53,458 : INFO : EPOCH 4 - PROGRESS: at 89.49% examples, 62929 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:54,704 : INFO : EPOCH 4 - PROGRESS: at 90.64% examples, 62827 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:56,031 : INFO : EPOCH 4 - PROGRESS: at 92.09% examples, 62830 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:57,300 : INFO : EPOCH 4 - PROGRESS: at 93.57% examples, 62874 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:58,331 : INFO : EPOCH 4 - PROGRESS: at 94.98% examples, 63024 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:50:59,448 : INFO : EPOCH 4 - PROGRESS: at 96.13% examples, 62940 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:00,470 : INFO : EPOCH 4 - PROGRESS: at 97.53% examples, 63082 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:01,600 : INFO : EPOCH 4 - PROGRESS: at 98.57% examples, 62984 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:02,461 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-09-15 03:51:02,536 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-09-15 03:51:02,659 : INFO : EPOCH 4 - PROGRESS: at 99.88% examples, 63082 words/s, in_qsize 1, out_qsize 1\n",
            "2019-09-15 03:51:02,662 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-09-15 03:51:02,666 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-09-15 03:51:02,667 : INFO : EPOCH - 4 : training on 8035381 raw words (5719332 effective words) took 90.6s, 63154 effective words/s\n",
            "2019-09-15 03:51:03,967 : INFO : EPOCH 5 - PROGRESS: at 1.11% examples, 49128 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:05,183 : INFO : EPOCH 5 - PROGRESS: at 2.53% examples, 58946 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:06,220 : INFO : EPOCH 5 - PROGRESS: at 4.07% examples, 63520 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:07,322 : INFO : EPOCH 5 - PROGRESS: at 5.25% examples, 63416 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:08,444 : INFO : EPOCH 5 - PROGRESS: at 6.36% examples, 61983 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:09,667 : INFO : EPOCH 5 - PROGRESS: at 7.81% examples, 63183 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:10,741 : INFO : EPOCH 5 - PROGRESS: at 8.96% examples, 63373 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:11,926 : INFO : EPOCH 5 - PROGRESS: at 10.18% examples, 62814 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:13,214 : INFO : EPOCH 5 - PROGRESS: at 11.70% examples, 63090 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:14,288 : INFO : EPOCH 5 - PROGRESS: at 13.04% examples, 63855 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:15,405 : INFO : EPOCH 5 - PROGRESS: at 14.04% examples, 63272 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:16,540 : INFO : EPOCH 5 - PROGRESS: at 15.12% examples, 62556 words/s, in_qsize 6, out_qsize 1\n",
            "2019-09-15 03:51:17,614 : INFO : EPOCH 5 - PROGRESS: at 16.25% examples, 62711 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:18,765 : INFO : EPOCH 5 - PROGRESS: at 17.35% examples, 62576 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:19,891 : INFO : EPOCH 5 - PROGRESS: at 18.69% examples, 62547 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:20,906 : INFO : EPOCH 5 - PROGRESS: at 19.93% examples, 62900 words/s, in_qsize 8, out_qsize 0\n",
            "2019-09-15 03:51:22,087 : INFO : EPOCH 5 - PROGRESS: at 21.18% examples, 62694 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:23,204 : INFO : EPOCH 5 - PROGRESS: at 22.52% examples, 63040 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:24,225 : INFO : EPOCH 5 - PROGRESS: at 23.65% examples, 62964 words/s, in_qsize 8, out_qsize 0\n",
            "2019-09-15 03:51:25,355 : INFO : EPOCH 5 - PROGRESS: at 24.98% examples, 63235 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:26,433 : INFO : EPOCH 5 - PROGRESS: at 26.01% examples, 62958 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:27,444 : INFO : EPOCH 5 - PROGRESS: at 27.41% examples, 63515 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:28,519 : INFO : EPOCH 5 - PROGRESS: at 28.60% examples, 63305 words/s, in_qsize 8, out_qsize 0\n",
            "2019-09-15 03:51:29,568 : INFO : EPOCH 5 - PROGRESS: at 29.73% examples, 63427 words/s, in_qsize 7, out_qsize 1\n",
            "2019-09-15 03:51:30,712 : INFO : EPOCH 5 - PROGRESS: at 30.87% examples, 63307 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:31,788 : INFO : EPOCH 5 - PROGRESS: at 31.94% examples, 63106 words/s, in_qsize 8, out_qsize 2\n",
            "2019-09-15 03:51:32,862 : INFO : EPOCH 5 - PROGRESS: at 33.29% examples, 63413 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:33,897 : INFO : EPOCH 5 - PROGRESS: at 34.41% examples, 63333 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:34,897 : INFO : EPOCH 5 - PROGRESS: at 35.66% examples, 63523 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:36,004 : INFO : EPOCH 5 - PROGRESS: at 36.92% examples, 63511 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:37,065 : INFO : EPOCH 5 - PROGRESS: at 38.10% examples, 63589 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:38,116 : INFO : EPOCH 5 - PROGRESS: at 39.31% examples, 63681 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:39,145 : INFO : EPOCH 5 - PROGRESS: at 40.44% examples, 63601 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:40,228 : INFO : EPOCH 5 - PROGRESS: at 41.69% examples, 63637 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:41,487 : INFO : EPOCH 5 - PROGRESS: at 43.03% examples, 63559 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:42,532 : INFO : EPOCH 5 - PROGRESS: at 44.35% examples, 63823 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:43,633 : INFO : EPOCH 5 - PROGRESS: at 45.50% examples, 63638 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:44,673 : INFO : EPOCH 5 - PROGRESS: at 46.69% examples, 63749 words/s, in_qsize 6, out_qsize 1\n",
            "2019-09-15 03:51:45,757 : INFO : EPOCH 5 - PROGRESS: at 47.94% examples, 63804 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:46,841 : INFO : EPOCH 5 - PROGRESS: at 49.15% examples, 63793 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:47,852 : INFO : EPOCH 5 - PROGRESS: at 50.42% examples, 63919 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:49,003 : INFO : EPOCH 5 - PROGRESS: at 51.69% examples, 63850 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:50,076 : INFO : EPOCH 5 - PROGRESS: at 52.81% examples, 63746 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:51,088 : INFO : EPOCH 5 - PROGRESS: at 54.16% examples, 64008 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:52,244 : INFO : EPOCH 5 - PROGRESS: at 55.31% examples, 63777 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:53,249 : INFO : EPOCH 5 - PROGRESS: at 56.37% examples, 63763 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:54,274 : INFO : EPOCH 5 - PROGRESS: at 57.54% examples, 63852 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:55,370 : INFO : EPOCH 5 - PROGRESS: at 58.59% examples, 63714 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:56,514 : INFO : EPOCH 5 - PROGRESS: at 59.83% examples, 63662 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:57,545 : INFO : EPOCH 5 - PROGRESS: at 61.03% examples, 63746 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:58,706 : INFO : EPOCH 5 - PROGRESS: at 62.26% examples, 63673 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:51:59,726 : INFO : EPOCH 5 - PROGRESS: at 63.51% examples, 63760 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:52:00,819 : INFO : EPOCH 5 - PROGRESS: at 64.70% examples, 63764 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:52:01,840 : INFO : EPOCH 5 - PROGRESS: at 65.75% examples, 63725 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:52:02,860 : INFO : EPOCH 5 - PROGRESS: at 66.85% examples, 63700 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:52:03,860 : INFO : EPOCH 5 - PROGRESS: at 68.11% examples, 63802 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:52:04,863 : INFO : EPOCH 5 - PROGRESS: at 69.25% examples, 63783 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:52:05,909 : INFO : EPOCH 5 - PROGRESS: at 70.37% examples, 63728 words/s, in_qsize 7, out_qsize 1\n",
            "2019-09-15 03:52:07,037 : INFO : EPOCH 5 - PROGRESS: at 71.70% examples, 63807 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:52:08,065 : INFO : EPOCH 5 - PROGRESS: at 72.92% examples, 63883 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:52:09,223 : INFO : EPOCH 5 - PROGRESS: at 74.07% examples, 63819 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:52:10,228 : INFO : EPOCH 5 - PROGRESS: at 75.37% examples, 63894 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:52:11,287 : INFO : EPOCH 5 - PROGRESS: at 76.62% examples, 63924 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:52:12,388 : INFO : EPOCH 5 - PROGRESS: at 77.90% examples, 63923 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:52:13,454 : INFO : EPOCH 5 - PROGRESS: at 79.10% examples, 63950 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:52:14,455 : INFO : EPOCH 5 - PROGRESS: at 80.14% examples, 63937 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:52:15,606 : INFO : EPOCH 5 - PROGRESS: at 81.54% examples, 63990 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:52:16,838 : INFO : EPOCH 5 - PROGRESS: at 82.93% examples, 63954 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:52:18,099 : INFO : EPOCH 5 - PROGRESS: at 84.44% examples, 64002 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:52:19,137 : INFO : EPOCH 5 - PROGRESS: at 85.51% examples, 63967 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:52:20,260 : INFO : EPOCH 5 - PROGRESS: at 86.80% examples, 64035 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:52:21,279 : INFO : EPOCH 5 - PROGRESS: at 87.87% examples, 64011 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:52:22,542 : INFO : EPOCH 5 - PROGRESS: at 89.27% examples, 63963 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:52:23,836 : INFO : EPOCH 5 - PROGRESS: at 90.64% examples, 63979 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:52:25,141 : INFO : EPOCH 5 - PROGRESS: at 92.09% examples, 63981 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:52:26,146 : INFO : EPOCH 5 - PROGRESS: at 93.43% examples, 64124 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:52:27,247 : INFO : EPOCH 5 - PROGRESS: at 94.62% examples, 64040 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:52:28,264 : INFO : EPOCH 5 - PROGRESS: at 96.00% examples, 64181 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:52:29,358 : INFO : EPOCH 5 - PROGRESS: at 97.17% examples, 64095 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:52:30,393 : INFO : EPOCH 5 - PROGRESS: at 98.46% examples, 64214 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-15 03:52:31,561 : INFO : EPOCH 5 - PROGRESS: at 99.51% examples, 64077 words/s, in_qsize 4, out_qsize 0\n",
            "2019-09-15 03:52:31,598 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-09-15 03:52:31,609 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-09-15 03:52:31,646 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-09-15 03:52:31,708 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-09-15 03:52:31,710 : INFO : EPOCH - 5 : training on 8035381 raw words (5722073 effective words) took 89.0s, 64268 effective words/s\n",
            "2019-09-15 03:52:31,711 : INFO : training on a 40176905 raw words (28605851 effective words) took 457.9s, 62474 effective words/s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 14min 57s, sys: 1.64 s, total: 14min 58s\n",
            "Wall time: 7min 40s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QUZHFsj8RaGo",
        "colab": {}
      },
      "source": [
        "def averaged_word2vec_vectorizer(corpus, model, num_features):\n",
        "    vocabulary = set(model.wv.index2word)\n",
        "    \n",
        "    def average_word_vectors(words, model, vocabulary, num_features):\n",
        "        feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
        "        nwords = 0.\n",
        "        \n",
        "        for word in words:\n",
        "            if word in vocabulary: \n",
        "                nwords = nwords + 1.\n",
        "                feature_vector = np.add(feature_vector, model.wv[word])\n",
        "        if nwords:\n",
        "            feature_vector = np.divide(feature_vector, nwords)\n",
        "\n",
        "        return feature_vector\n",
        "\n",
        "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
        "                    for tokenized_sentence in corpus]\n",
        "    return np.array(features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zWfcUVixRaGr",
        "colab": {}
      },
      "source": [
        "# generate averaged word vector features from word2vec model\n",
        "avg_wv_train_features = averaged_word2vec_vectorizer(corpus=tokenized_train, model=w2v_model,\n",
        "                                                     num_features=w2v_num_features)\n",
        "avg_wv_test_features = averaged_word2vec_vectorizer(corpus=tokenized_test, model=w2v_model,\n",
        "                                                    num_features=w2v_num_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3GMe1dCfRaGw",
        "outputId": "a6913a2e-02f8-4097-b237-af4af6f0fbb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Word2Vec model:> Train features shape:', avg_wv_train_features.shape, ' Test features shape:', avg_wv_test_features.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word2Vec model:> Train features shape: (35000, 300)  Test features shape: (15000, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8mSFQ9H0RaGy"
      },
      "source": [
        "## Modeling with deep neural networks "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-oz9gtU9RaGz"
      },
      "source": [
        "### Building Deep neural network architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eCI0uEBIRaGz",
        "colab": {}
      },
      "source": [
        "def construct_deepnn_architecture(num_input_features):\n",
        "    dnn_model = Sequential()\n",
        "    dnn_model.add(Dense(512, input_shape=(num_input_features,)))\n",
        "    dnn_model.add(Activation('relu'))\n",
        "    dnn_model.add(Dropout(0.2))\n",
        "    \n",
        "    dnn_model.add(Dense(256))\n",
        "    dnn_model.add(Activation('relu'))\n",
        "    dnn_model.add(Dropout(0.2))\n",
        "    \n",
        "    dnn_model.add(Dense(256))\n",
        "    dnn_model.add(Activation('relu'))\n",
        "    dnn_model.add(Dropout(0.2))\n",
        "    \n",
        "    dnn_model.add(Dense(1))\n",
        "    dnn_model.add(Activation('sigmoid'))\n",
        "\n",
        "    dnn_model.compile(loss='binary_crossentropy', optimizer='adam',                 \n",
        "                      metrics=['accuracy'])\n",
        "    return dnn_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3cQpTH6FRaG1",
        "outputId": "125d1646-5597-48a2-f395-e6fa00be57bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "w2v_dnn = construct_deepnn_architecture(num_input_features=w2v_num_features)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-15 04:03:08,640 : WARNING : From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-15 04:03:09,606 : WARNING : From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GBzbo-YlRaG4"
      },
      "source": [
        "### Visualize sample deep architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0fuhzYagRaG5",
        "outputId": "88ab7094-595b-4510-a8f1-2002dd0ed4e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "w2v_dnn.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 512)               154112    \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 257       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 351,489\n",
            "Trainable params: 351,489\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s-COslmPRaG8"
      },
      "source": [
        "### Model Training, Prediction and Performance Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0kcisAskRaG8",
        "outputId": "bd3ecd7a-d670-4bb8-ab32-7a46460dbf74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "source": [
        "batch_size = 100\n",
        "w2v_dnn.fit(avg_wv_train_features, y_train, epochs=10, batch_size=batch_size, \n",
        "            shuffle=True, validation_split=0.1, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 31500 samples, validate on 3500 samples\n",
            "Epoch 1/10\n",
            "31500/31500 [==============================] - 2s 77us/sample - loss: 0.3239 - acc: 0.8632 - val_loss: 0.2999 - val_acc: 0.8789\n",
            "Epoch 2/10\n",
            "31500/31500 [==============================] - 1s 47us/sample - loss: 0.2921 - acc: 0.8806 - val_loss: 0.3260 - val_acc: 0.8597\n",
            "Epoch 3/10\n",
            "31500/31500 [==============================] - 2s 50us/sample - loss: 0.2843 - acc: 0.8834 - val_loss: 0.3017 - val_acc: 0.8763\n",
            "Epoch 4/10\n",
            "31500/31500 [==============================] - 2s 49us/sample - loss: 0.2778 - acc: 0.8856 - val_loss: 0.2946 - val_acc: 0.8800\n",
            "Epoch 5/10\n",
            "31500/31500 [==============================] - 2s 48us/sample - loss: 0.2691 - acc: 0.8898 - val_loss: 0.2993 - val_acc: 0.8817\n",
            "Epoch 6/10\n",
            "31500/31500 [==============================] - 2s 49us/sample - loss: 0.2640 - acc: 0.8912 - val_loss: 0.2999 - val_acc: 0.8823\n",
            "Epoch 7/10\n",
            "31500/31500 [==============================] - 2s 48us/sample - loss: 0.2556 - acc: 0.8943 - val_loss: 0.3093 - val_acc: 0.8760\n",
            "Epoch 8/10\n",
            "31500/31500 [==============================] - 2s 49us/sample - loss: 0.2454 - acc: 0.8990 - val_loss: 0.3057 - val_acc: 0.8806\n",
            "Epoch 9/10\n",
            "31500/31500 [==============================] - 2s 50us/sample - loss: 0.2389 - acc: 0.9003 - val_loss: 0.3006 - val_acc: 0.8786\n",
            "Epoch 10/10\n",
            "31500/31500 [==============================] - 2s 48us/sample - loss: 0.2298 - acc: 0.9053 - val_loss: 0.3218 - val_acc: 0.8757\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f96a1233438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7C1gcc4iRaG-",
        "outputId": "1d6e244f-9132-48eb-fb95-5fa666dc8076",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "y_pred = w2v_dnn.predict_classes(avg_wv_test_features)\n",
        "predictions = le.inverse_transform(y_pred) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:273: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2XHEZ-X9RaG_",
        "outputId": "60bedb6e-2a55-4787-a676-8a358f74187e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "labels = ['negative', 'positive']\n",
        "print(classification_report(test_sentiments, predictions))\n",
        "pd.DataFrame(confusion_matrix(test_sentiments, predictions), index=labels, columns=labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.85      0.91      0.88      7490\n",
            "    positive       0.90      0.84      0.87      7510\n",
            "\n",
            "    accuracy                           0.87     15000\n",
            "   macro avg       0.88      0.87      0.87     15000\n",
            "weighted avg       0.88      0.87      0.87     15000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>negative</th>\n",
              "      <th>positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>negative</th>\n",
              "      <td>6801</td>\n",
              "      <td>689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <td>1207</td>\n",
              "      <td>6303</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          negative  positive\n",
              "negative      6801       689\n",
              "positive      1207      6303"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZdyWLTE3-TA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}